{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd189ee7-c99c-492a-b9a0-b5c228bf4449",
   "metadata": {},
   "outputs": [],
   "source": [
    "                                                       DESCION TREE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21b88ef-7ba8-41ea-aca9-ee4a9969f0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Question 1: What is a Decision Tree, and how does it work in the context of classfication?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65985e8-fd32-47be-b7c9-5ecd278ea59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "A decision tree is a **supervised learning algorithm** that predicts an outcome by splitting data into branches based on feature values.\n",
    "\n",
    "In classification, it works by:\n",
    "\n",
    "* Starting at a root node with the whole dataset.\n",
    "* Splitting the data using rules (like *Gini impurity* or *entropy*) to create homogeneous subsets.\n",
    "* Repeating the process until leaf nodes represent class labels or stopping criteria are met.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1c8013-d488-4a95-955a-913a6f906143",
   "metadata": {},
   "outputs": [],
   "source": [
    "Question 2: Explain the concepts of Gini Impurity and Entropy as impurity measures.\n",
    "How do they impact the splits in a Decision Tree?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40632e9-02d9-463f-9a36-d3f3b05676b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "**Gini Impurity** and **Entropy** are metrics used to measure how “pure” a node is in a decision tree.\n",
    "\n",
    "* **Gini Impurity**\n",
    "\n",
    "  * Formula: $Gini = 1 - \\sum p_i^2$\n",
    "  * Measures the probability that a randomly chosen sample would be misclassified if randomly labeled according to the class distribution in the node.\n",
    "  * Lower Gini → more pure node.\n",
    "\n",
    "* **Entropy**\n",
    "\n",
    "  * Formula: $Entropy = -\\sum p_i \\log_2(p_i)$\n",
    "  * Measures the amount of uncertainty or disorder in a node.\n",
    "  * Lower entropy → more pure node.\n",
    "\n",
    "**Impact on Splits**:\n",
    "\n",
    "* Decision trees choose splits that **minimize impurity** (Gini or Entropy).\n",
    "* Both often give similar results: Gini is faster to compute, while entropy is based on information theory and can be more sensitive to class imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3b32d0-0558-4e0a-8810-9b428cc36104",
   "metadata": {},
   "outputs": [],
   "source": [
    "Question 3: What is the difference between Pre-Pruning and Post-Pruning in Decision\n",
    "Trees? Give one practical advantage of using each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf6265b-5f80-44dc-bc7b-c2918a6df510",
   "metadata": {},
   "outputs": [],
   "source": [
    "**Difference between Pre-Pruning and Post-Pruning in Decision Trees**\n",
    "\n",
    "| Aspect           | Pre-Pruning (Early Stopping)                                                                 | Post-Pruning (Prune After Growth)                                                            |\n",
    "| ---------------- | -------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------- |\n",
    "| **When Applied** | Stops tree growth early, before it becomes too complex.                                      | Grows the full tree first, then removes unnecessary branches.                                |\n",
    "| **Method**       | Uses constraints like maximum depth, minimum samples per leaf, or minimum impurity decrease. | Uses validation data or cost-complexity pruning to cut branches that don’t improve accuracy. |\n",
    "| **Goal**         | Prevent overfitting from the start.                                                          | Simplify an already overfit tree to improve generalization.                                  |\n",
    "\n",
    "**Practical Advantage**\n",
    "\n",
    "* **Pre-Pruning Advantage:** Saves computational time and memory since the tree is never allowed to grow too large.\n",
    "* **Post-Pruning Advantage:** Often yields better accuracy than pre-pruning because it considers the full picture before deciding what to cut.\n",
    "\n",
    "If you want, I can also give you a **real-world example** where both are applied differently in practice.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3421c7cd-cdde-488b-ac13-74b28d091a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Question 4: What is Information Gain in Decision Trees, and why is it important for\n",
    "choosing the best split?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e16631b-1053-4d98-94b5-3d5f12311836",
   "metadata": {},
   "outputs": [],
   "source": [
    "**Information Gain (IG) in Decision Trees**\n",
    "\n",
    "* **Definition:**\n",
    "  Information Gain measures the reduction in impurity (or uncertainty) about the target variable after splitting the data on a specific feature.\n",
    "  It’s calculated as:\n",
    "\n",
    "$$\n",
    "IG = \\text{Impurity before split} - \\text{Weighted impurity after split}\n",
    "$$\n",
    "\n",
    "Where impurity is often measured using **Entropy** or **Gini Index**.\n",
    "\n",
    "---\n",
    "\n",
    "**Why It’s Important for Choosing the Best Split:**\n",
    "\n",
    "* Decision Trees work by dividing data into smaller, purer subsets.\n",
    "* IG quantifies **how much a split improves the “purity” of the target classes**.\n",
    "* The higher the IG, the better the split, because it means the feature is more informative about the target.\n",
    "* Without IG (or a similar measure), the tree might choose irrelevant features, leading to poor predictions and overfitting.\n",
    "\n",
    "---\n",
    "\n",
    "**Example:**\n",
    "If splitting by “Age < 30” reduces the class mixing more than splitting by “Income > 50K,” then **Age** would be chosen as the splitting feature at that node because it has higher IG.\n",
    "\n",
    "If you’d like, I can also **show you a quick numeric example** of IG calculation so it’s crystal clear.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177ba106-41f1-4c75-b5b3-ecfae6276e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Question 5: What are some common real-world applications of Decision Trees, and\n",
    "what are their main advantages and limitations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f1a1c9-230e-44f8-9f54-7bd321cf60d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "**Common Real-World Applications of Decision Trees**\n",
    "\n",
    "1. **Customer Segmentation & Marketing**\n",
    "\n",
    "   * Predicting which customers are likely to buy a product.\n",
    "   * Example: Targeting high-value customers for promotions.\n",
    "\n",
    "2. **Fraud Detection**\n",
    "\n",
    "   * Classifying transactions as legitimate or fraudulent.\n",
    "   * Example: Credit card companies flagging suspicious purchases.\n",
    "\n",
    "3. **Medical Diagnosis**\n",
    "\n",
    "   * Assisting doctors in predicting diseases based on symptoms and test results.\n",
    "\n",
    "4. **Loan Approval & Credit Scoring**\n",
    "\n",
    "   * Deciding whether to approve a loan based on applicant’s income, credit history, etc.\n",
    "\n",
    "5. **Manufacturing & Quality Control**\n",
    "\n",
    "   * Identifying defects or causes of machine failures.\n",
    "\n",
    "---\n",
    "\n",
    "**Main Advantages**\n",
    "\n",
    "* **Easy to Interpret:** Visual and understandable even for non-technical stakeholders.\n",
    "* **Handles Both Numerical & Categorical Data:** Flexible with different data types.\n",
    "* **No Feature Scaling Needed:** Works without normalization or standardization.\n",
    "* **Captures Non-Linear Relationships:** Can model complex decision boundaries.\n",
    "\n",
    "---\n",
    "\n",
    "**Main Limitations**\n",
    "\n",
    "* **Overfitting:** Trees can grow too complex without pruning.\n",
    "* **Instability:** Small changes in data can lead to different splits.\n",
    "* **Biased Towards Features with More Levels:** Categorical variables with many categories may be chosen more often.\n",
    "* **Lower Predictive Accuracy:** Often less accurate than ensemble methods like Random Forests or Gradient Boosted Trees.\n",
    "\n",
    "---\n",
    "\n",
    "If you want, I can also **summarize this into a neat table** so it’s exam-friendly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453a4f8f-0ee4-4c63-9f40-9824ff7e3fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset Info:\n",
    "● Iris Dataset for classification tasks (sklearn.datasets.load_iris() or\n",
    "provided CSV).\n",
    "● Boston Housing Dataset for regression tasks\n",
    "(sklearn.datasets.load_boston() or provided CSV).\n",
    "Question 6: Write a Python program to:\n",
    "● Load the Iris Dataset\n",
    "● Train a Decision Tree Classifier using the Gini criterion\n",
    "● Print the model’s accuracy and feature importances\n",
    "(Include your Python code and output in the code box below.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4574cfc0-0b38-479e-937d-a308720311f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 1.0\n",
      "Feature Importances:\n",
      "sepal length (cm): 0.0000\n",
      "sepal width (cm): 0.0167\n",
      "petal length (cm): 0.9061\n",
      "petal width (cm): 0.0772\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data       # Features\n",
    "y = iris.target     # Target labels\n",
    "\n",
    "# Split into train and test sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Create Decision Tree Classifier using Gini criterion\n",
    "clf = DecisionTreeClassifier(criterion=\"gini\", random_state=42)\n",
    "\n",
    "# Train the model\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Print results\n",
    "print(\"Model Accuracy:\", accuracy)\n",
    "print(\"Feature Importances:\")\n",
    "for feature_name, importance in zip(iris.feature_names, clf.feature_importances_):\n",
    "    print(f\"{feature_name}: {importance:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b14e1e9-4e31-4375-ae24-fefc1f12e28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Question 7: Write a Python program to:\n",
    "● Load the Iris Dataset\n",
    "● Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to\n",
    "a fully-grown tree.\n",
    "(Include your Python code and output in the code box below.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0b8211c-53d0-4374-8963-0b291f0ea370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (max_depth=3): 1.0000\n",
      "Accuracy (Full Tree):   1.0000\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Model 1: Decision Tree with max_depth=3 (Pre-Pruned)\n",
    "clf_depth3 = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
    "clf_depth3.fit(X_train, y_train)\n",
    "y_pred_depth3 = clf_depth3.predict(X_test)\n",
    "accuracy_depth3 = accuracy_score(y_test, y_pred_depth3)\n",
    "\n",
    "# Model 2: Fully-grown Decision Tree\n",
    "clf_full = DecisionTreeClassifier(random_state=42)\n",
    "clf_full.fit(X_train, y_train)\n",
    "y_pred_full = clf_full.predict(X_test)\n",
    "accuracy_full = accuracy_score(y_test, y_pred_full)\n",
    "\n",
    "# Print comparison\n",
    "print(f\"Accuracy (max_depth=3): {accuracy_depth3:.4f}\")\n",
    "print(f\"Accuracy (Full Tree):   {accuracy_full:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97486027-bab0-4d65-a77d-5660ecdf23d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Question 8: Write a Python program to:\n",
    "● Load the California Housing dataset from sklearn\n",
    "● Train a Decision Tree Regressor\n",
    "● Print the Mean Squared Error (MSE) and feature importances\n",
    "(Include your Python code and output in the code box below.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e0f386-fd43-4cd8-9964-179f2ad85781",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load data\n",
    "X, y = fetch_california_housing(return_X_y=True)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Decision Tree Regressor\n",
    "reg = DecisionTreeRegressor(random_state=42)\n",
    "reg.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "mse = mean_squared_error(y_test, reg.predict(X_test))\n",
    "print(\"MSE:\", mse)\n",
    "print(\"Feature Importances:\", reg.feature_importances_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722cc362-6eea-4ebc-b0b9-e5b22e736b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "Question 9: Write a Python program to:\n",
    "● Load the Iris Dataset\n",
    "● Tune the Decision Tree’s max_depth and min_samples_split using\n",
    "GridSearchCV\n",
    "● Print the best parameters and the resulting model accuracy\n",
    "(Include your Python code and output in the code box below.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9430f3a-f5b6-4a8d-811d-5c0b94a3059f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Create a Decision Tree Classifier\n",
    "clf = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'max_depth': [2, 3, 4, 5, None],\n",
    "    'min_samples_split': [2, 3, 4, 5]\n",
    "}\n",
    "\n",
    "# Perform Grid Search with cross-validation\n",
    "grid_search = GridSearchCV(clf, param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "\n",
    "# Evaluate the best model\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Model Accuracy:\", accuracy)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
